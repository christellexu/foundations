import numpy as np
import pandas as pd
import statsmodels.formula.api as sm
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential
from keras.layers.core import Dense


#-----Simulating Linear Data-----

#-----messier-----
# x1 = 2.5 * np.random.randn(100) + 3
# x2 = 2.5 * np.random.randn(100) + 3
#
# # x = np.linspace(0, 100, 1000)
# epsilon = np.random.randn(100)
# y = 5 + .6 * x1 + 0.3 * x2 + epsilon
#
#
# plt.scatter(x1, y)
# # plt.plot(x, y)

# #cleaner
# x = np.arange(100)
# epsilon = np.random.uniform(-10, 10, size=(100,))
# y = 5 + .6 * x + epsilon
#
# plt.scatter(x, y)
# # plt.plot(x, y)


np.random.seed(seed=1)
# np.random.seed(seed=8)


# Code taken from http://peterroelants.github.io/posts/neural_network_implementation_part01/
#----Defining vector of input samples-----
x = np.random.uniform(0, 1, 20)

#-----Generating target values with noise-----
def f(x):
    return x * 2

noise_variance = 0.2 #so things don't fit perfectly
noise = np.random.randn(x.shape[0]) * noise_variance

t = f(x) + noise

# plt.plot(x, t, "o")
# plt.plot([0,1], [f(0), f(1)])

#-----Define neural net function-----
def neuralnet(x, w):
    return x * w #in this case w is a scalar

#-----Define cost function-----
def cost(y, t):
    return ((t-y)**2).sum()

#-----Gradient / Partial of loss wrt weight-----
def gradient(w, x, t):
    return 2 * x * (neuralnet(x, w) - t)

#-----Delta w-----
def delta_w(w, x, t, learning_rate):
    return learning_rate * gradient(w, x, t).sum()

#-----Set initial weight parameter------
w = 0.1

#-----Set learning rate-----
#It's worth noting that when I increased the sample size to 40, my gradient exploded
#until I decreased the learning rate
#https://stackoverflow.com/questions/19928068/neural-network-weights-explode-in-linear-unit
#I could have also decreased the initial weight
learning_rate = 0.1


#-----Updates-----
num_iterations = 200 #number of updates, greater the number, the closer to the true value

w_cost = [(w, cost(neuralnet(x,w), t))] # List to store weights and cost values

for i in range(num_iterations):
    dw = delta_w(w, x, t, learning_rate) #defines the update
    w = w - dw #updates weight
    w_cost.append((w, cost(neuralnet(x, w), t)))


for i in range(num_iterations):
    dw = delta_w(w, x, t, learning_rate) #defines the update
    w = w - dw #updates weight
    w_cost.append((w, cost(neuralnet(x, w), t)))

#-----OLS-----
# https://stackoverflow.com/questions/19991445/run-an-ols-regression-with-pandas-data-frame
df = pd.DataFrame({"A": t, "B": x})
result = sm.ols(formula="A ~B", data = df).fit()

print result.params
B = result.params[1]


#-----tensorflow------
# n_dim = x.shape[0]
# training_epochs = 50
# cost_history = np.empty(shape=[1],dtype=float)
#
# X = tf.placeholder(tf.float32,[None,n_dim])
# Y = tf.placeholder(tf.float32)
# W = tf.Variable(tf.ones([n_dim,1]))
#
# init = tf.initialize_all_variables()
#
# y_ = tf.matmul(X, W)
# cost = tf.reduce_mean(tf.square(y_ - Y))
# training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
#
# sess = tf.Session()
# sess.run(init)
#
# for epoch in range(training_epochs):
#     sess.run(training_step, feed_dict={X:t(x), Y:t})
#     cost_history = np.append(cost_history, sess.run(cost, feed_dict={X:x, Y:t}))

#-----Keras----- Fickle
sgd = keras.optimizers.SGD(lr=0.3, momentum=0.0, decay=0.0, nesterov=False) #n = 20

#lr = 0.1 0.0394
#lr = 0.2 0.0393
#lr = 0.3 0.0393
#lr = 0.4 0.0393
#lr = 0.5 0.0393
#lr = 0.6 0.0393
#lr = 0.7 0.0393
#lr = 0.8 NA

def baseline_model():
    model = Sequential()
    model.add(Dense(1, input_dim=1, activation='linear'))
    model.compile(loss="mean_squared_error", optimizer=sgd)
    return model

regression = baseline_model()
regression.fit(x, t, nb_epoch=200)
regression.get_weights()



#-----Compare-----
plt.plot(x, t, "o", label = "t")
plt.plot([0,1], [f(0), f(1)], label = "f(x)")
plt.plot([0,1], [0*w, 1*w], label = "NN fitted")
plt.plot([0,1], [0*B, 1*B], label = "OLS")
plt.plot(x, regression.predict(x), label = "Keras NN")
plt.legend()
