import numpy as np
import pandas as pd
import statsmodels.formula.api as sm
import matplotlib.pyplot as plt
import tensorflow as tf
import statsmodels.api as sm
import scipy




#-----Simulating Linear Data-----
# Code taken from http://peterroelants.github.io/posts/neural_network_implementation_part01/

# np.random.seed(seed=1)
np.random.seed(seed=8)

#----Defining vector of input samples-----
# x = np.random.uniform(0, 1, 20)
x = np.random.uniform(0, 1, 40)

#-----Generating target values with noise-----
def f(x):
    return x * 2

noise_variance = 0.2 #so things don't fit perfectly
noise = np.random.randn(x.shape[0]) * noise_variance

t = f(x) + noise

# outlier if seed = 1
# t[15] = 0.2 * t[15]

# outlier if seed = 8
# t[1] = 0.2 * t[1] #may have to try multiple outliers

for i in xrange(len(t)):
    if np.random.random() < 0.1:
        t[i] += 2

# plt.scatter(x, t)


# plt.plot(x, t, "o")
# plt.plot([0,1], [f(0), f(1)])

#-----Define neural net function-----
def neuralnet(x, w):
    return x * w #in this case w is a scalar

#-----Define cost function-----
def cost(y, t):
    return ((t-y)**2).sum()


l = np.zeros(len(t))
def huber(y, t, c):
    for i in t:
        return .5 * (t - y) ** 2 if np.all(abs((t - y) <= c)) else c * (2 * abs(t - y) - c)



#-----LS Gradient / Partial of loss wrt weight-----
def gradient(w, x, t):
    return 2 * x * (neuralnet(x, w) - t)

#-----LS Delta w-----
def delta_w(w, x, t, learning_rate):
    return learning_rate * gradient(w, x, t).sum()

#-----Set initial weight parameter------
w = 0.1
w_hub = 0.1


#-----Set learning rate-----
# learning_rate = 0.1 #n = 20
learning_rate = 0.01

#-----LS NN Updates-----
num_iterations = 200 #number of updates, greater the number, the closer to the true value

w_cost = [(w, cost(neuralnet(x,w), t))] # List to store weights and cost values

for i in range(num_iterations):
    dw = delta_w(w, x, t, learning_rate) #defines the update
    w = w - dw #updates weight
    w_cost.append((w, cost(neuralnet(x, w), t)))

#-----Huber Gradient----- you still need to change f(x) into nn
# def huber_gradient(w_hub, x, t, c):
#     for i in x:
#         return x * (t - neuralnet(x, w_hub)) if np.all(abs(t - neuralnet(x, w_hub))) <= c else x * c * np.sign(t - neuralnet(x, w_hub))

# r = t - neuralnet(x, w_hub)

# def huber_gradient(x,r,c):
#     for i in x:
#         return x * (r) if np.all(abs(r)) <= c else x * c * np.sign(r)


def huber_gradient(w_hub, x, t, c):
    for i in x:
        return x * (neuralnet(x, w_hub) - t) if np.all(abs(t - neuralnet(x, w_hub))) <= c else x * c * np.sign(
            neuralnet(x, w_hub) - t)


#-----Huber Delta-----
# def hub_delta(w_hub, x, t, learning_rate):
#     return learning_rate * huber_gradient(x, r, c).sum()
#
def hub_delta(w_hub, x, t, learning_rate):
    return learning_rate * huber_gradient(w_hub, x, t, c).sum()

#-----Set initial weight parameter------
w_hub = 0.1
learning_rate = 0.01
c = 0.7 #what is the best way to choose this?

#-----Experimenting with formula-----
scipy.special.huber(1, t - neuralnet(x, w_hub))

#-----Huber NN Updates-----
num_iterations = 200
w_h_cost = [(w_hub, huber(neuralnet(x,w_hub), t, c))] # List to store weights and cost values
# w_h_cost = [(w, scipy.special.huber(1, t - neuralnet(x, w)))] # List to store weights and cost values

for i in range(num_iterations):
    dw_hub = hub_delta(w_hub, x, t, learning_rate)
    w_hub = w_hub - dw_hub
    w_h_cost.append((w_hub, huber(neuralnet(x, w_hub), t, c)))
    # print w_h_cost

# for i in range(num_iterations):
#     dw_hub = hub_delta(w_hub, x, t, learning_rate)
#     w_hub = w_hub - dw_hub
#     w_h_cost.append((w_hub, huber(neuralnet(x, w_hub), t, c)))
#     print w_h_cost

# for i in range(num_iterations):
#     dw = hub_delta(w, x, t, learning_rate)
#     w = w - dw
#     w_h_cost.append((w, scipy.special.huber(1, t - neuralnet(x, w))))
#     print w_h_cost

#-----Regression Huber-----
#http://www.statsmodels.org/dev/rlm.html
rlm_model = sm.RLM(t, x, M=sm.robust.norms.HuberT()) #M specifies the method for downweighting outliers
rlm_results = rlm_model.fit()
print(rlm_results.params)
w_ols = float(rlm_results.params)



#-----Tensorflow-----
#https://github.com/nlintz/TensorFlow-Tutorials/blob/master/01_linear_regression.ipynb
X = tf.placeholder('float')
Y = tf.placeholder('float')



def model(X, w_tf):
    return tf.multiply(X, w_tf)

w_tf = tf.Variable(0.1, name = 'weights')
y_model = model(X, w_tf)

# cost = tf.square(Y - y_model)
# train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost)

#this part had to be solved in Terminal which supports tf 1.2.0, and for some reason
#my console only suppors 1.1.0
loss = tf.losses.huber_loss(Y, y_model, delta = 0.5)
train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)


with tf.Session() as sess:
    tf.global_variables_initializer().run()
    for i in range(num_iterations):
        for (i, j) in zip(x, t):
            sess.run(train_op, feed_dict={X:i, Y:j})
    print(sess.run(w_tf))

#manually input the output value  until I figure out how to do it automatically
w_tf = 2.16952


#-----Compare-----
plt.plot(x, t, "o", label = "t")
plt.plot([0,1], [f(0), f(1)], label = "f(x)")
plt.plot([0,1], [0*w_hub, 1*w_hub], label = "NN fitted with huber")
plt.plot([0,1], [0*w_ols, 1*w_ols], label = "Regression Huber")
plt.plot([0,1], [0*w_tf, 1*w_tf], label = "TensorFlow NN Huber")
plt.legend()

#essentially, the closer w is to 2, the closer the line is to the true fitted line
