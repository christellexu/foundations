import numpy as np
import pandas as pd
import statsmodels.formula.api as sm
import matplotlib.pyplot as plt
import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers.core import Dense

#-----Simulating Linear Data-----
# np.random.seed(seed=1)
np.random.seed(seed=8)


# Code taken from http://peterroelants.github.io/posts/neural_network_implementation_part01/
#----Defining vector of input samples-----
x = np.random.uniform(0, 1, 40)

#-----Generating target values with noise-----
def f(x):
    return x * 2

noise_variance = 0.2 #so things don't fit perfectly
noise = np.random.randn(x.shape[0]) * noise_variance

t = f(x) + noise

# plt.plot(x, t, "o")
# plt.plot([0,1], [f(0), f(1)])

#-----Define neural net function-----
def neuralnet(x, w):
    return x * w #in this case w is a scalar

#-----Define cost function-----
def cost(y, t):
    return ((t-y)**2).sum()

#-----Gradient / Partial of loss wrt weight-----
def gradient(w, x, t):
    return 2 * x * (neuralnet(x, w) - t)

#-----Delta w-----
def delta_w(w, x, t, learning_rate):
    return learning_rate * gradient(w, x, t).sum()

#-----Set initial weight parameter------
w = 0.1

#-----Set learning rate-----
#It's worth noting that when I increased the sample size to 40, my gradient exploded
#until I decreased the learning rate
#https://stackoverflow.com/questions/19928068/neural-network-weights-explode-in-linear-unit
#I could have also decreased the initial weight
# learning_rate = 0.1 #when sample size is 20
learning_rate = 0.01

#-----Updates-----
num_iterations = 200 #number of updates, greater the number, the closer to the true value

w_cost = [(w, cost(neuralnet(x,w), t))] # List to store weights and cost values

for i in range(num_iterations):
    dw = delta_w(w, x, t, learning_rate) #defines the update
    w = w - dw #updates weight
    w_cost.append((w, cost(neuralnet(x, w), t)))


#-----OLS-----
# https://stackoverflow.com/questions/19991445/run-an-ols-regression-with-pandas-data-frame
df = pd.DataFrame({"A": t, "B": x})
result = sm.ols(formula="A ~B", data = df).fit()

print result.params
B = result.params[1]

#-----Keras----- Fickle
# sgd = keras.optimizers.SGD(lr=0.3, momentum=0.0, decay=0.0, nesterov=False) #n = 20
sgd = keras.optimizers.SGD(lr=0.3, momentum=0.0, decay=0.0, nesterov=False) #n = 40

#lr = 0.1 0.0394
#lr = 0.2 0.0393
#lr = 0.3 0.0393
#lr = 0.4 0.0393
#lr = 0.5 0.0393
#lr = 0.6 0.0393
#lr = 0.7 0.0393
#lr = 0.8 NA

def baseline_model():
    model = Sequential()
    model.add(Dense(1, input_dim=1, activation='linear'))
    model.compile(loss="mean_squared_error", optimizer=sgd)
    return model

regression = baseline_model()
regression.fit(x, t, nb_epoch=200)
regression.get_weights()

#-----Tensorflow-----
#https://github.com/nlintz/TensorFlow-Tutorials/blob/master/01_linear_regression.ipynb
X = tf.placeholder('float')
Y = tf.placeholder('float')

def model(X, w_tf):
    return tf.multiply(X, w_tf)

w_tf = tf.Variable(0.1, name = 'weights')
y_model = model(X, w_tf)

cost = tf.square(Y - y_model)
train_op = tf.train.GradientDescentOptimizer(0.01).minimize(cost)

with tf.Session() as sess:
    tf.global_variables_initializer().run()

    for i in range(num_iterations):
        for (i, j) in zip(x, t):
            sess.run(train_op, feed_dict={X:i, Y:j})

    print(sess.run(w_tf))

#manually input the output value  until I figure out how to do it automatically
w_tf = 2.0109


#-----Compare-----
plt.plot(x, t, "o", label = "t")
plt.plot([0,1], [f(0), f(1)], label = "f(x)")
plt.plot([0,1], [0*w, 1*w], label = "NN fitted")
plt.plot([0,1], [0*B, 1*B], label = "OLS")
plt.plot(x, regression.predict(x), label = "Keras NN")
plt.plot([0,1], [0*w_tf, 1*w_tf], label = "TensorFlow NN")
plt.legend()
